import streamlit as st
import openai
from streamlit_mic_recorder import mic_recorder
from PIL import Image
import os
import base64
import edge_tts
import asyncio
import nest_asyncio

# Apply nested asyncio loop
nest_asyncio.apply()

# Set OpenAI API key from secrets
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Google Speech or Edge
GOOGLE_KEY = st.secrets["GOOGLE_API_KEY"]
AZURE_KEY = st.secrets["AZURE_SPEECH_KEY"]
AZURE_REGION = st.secrets["AZURE_REGION"]

# Page settings
st.set_page_config(
    page_title="ê³µìŒ¤ â€“ ì´ˆê°œì¸í™” í•™ìŠµ ìƒë‹´",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS
st.markdown("""
<style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* Main container styling */
    .main {
        padding: 2rem;
        background-color: #f8f9fa;
    }
    
    /* Title styling */
    h1 {
        color: #2c3e50;
        font-size: 2.5rem !important;
        font-weight: 700 !important;
        margin-bottom: 1rem !important;
    }
    
    /* Subtitle styling */
    .subtitle {
        color: #7f8c8d;
        font-size: 1.2rem !important;
        margin-bottom: 2rem !important;
    }
    
    /* Card styling */
    .stButton>button {
        width: 100%;
        background-color: #3498db;
        color: white;
        border: none;
        padding: 0.5rem 1rem;
        border-radius: 5px;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    
    .stButton>button:hover {
        background-color: #2980b9;
        transform: translateY(-2px);
    }
    
    /* Input styling */
    .stTextInput>div>div>input {
        border-radius: 5px;
        border: 2px solid #e0e0e0;
        padding: 0.5rem;
    }
    
    /* Radio button styling */
    .stRadio>div {
        background-color: white;
        padding: 1rem;
        border-radius: 5px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    /* Response container styling */
    .response-container {
        background-color: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin-top: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.title("ğŸ‘©â€ğŸ« ê³µìŒ¤ê³¼ í•¨ê»˜í•˜ëŠ” í•™ìŠµ ìƒë‹´")
st.markdown('<p class="subtitle">ğŸ§  ë‚˜ì—ê²Œ ê¼­ ë§ëŠ” ê³µë¶€ ë°©ë²•ì„ AI ì„ ìƒë‹˜ ê³µìŒ¤ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!</p>', unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœì— ìŒì„± ì„¤ì • ì €ì¥
if "saved_voice" not in st.session_state:
    st.session_state["saved_voice"] = {
        "lang": "ko-KR",
        "gender": "ì—¬ì„±",
        "speed": 0
    }

# Sidebar for user profile
with st.sidebar:
    st.header("ğŸ‘¤ í•™ìŠµì ì •ë³´")
    name = st.text_input("ì´ë¦„")
    selected_level = st.radio(
        "í•™ìŠµì ìˆ˜ì¤€ì„ ì„ íƒí•´ì£¼ì„¸ìš”",
        ["ìœ ì¹˜ì›", "ì´ˆë“±í•™ìƒ", "ì¤‘í•™ìƒ", "ê³ ë“±í•™ìƒ"],
        horizontal=True,
        label_visibility="collapsed"
    )
    subject = st.multiselect("ê´€ì‹¬ ê³¼ëª©", ["ìˆ˜í•™", "ê³¼í•™", "êµ­ì–´", "ì˜ì–´"])
    mood = st.radio("ì˜¤ëŠ˜ ê¸°ë¶„ì€?", ["ğŸ™‚ ê´œì°®ì•„ìš”", "ğŸ˜ ë³´í†µì´ì—ìš”", "ğŸ˜£ ì¢€ í˜ë“¤ì–´ìš”"])
    st.markdown("---")
    st.subheader("ğŸ”ˆ ìŒì„± ì¶œë ¥ ì„¤ì •")
    voice_lang = st.selectbox("ìŒì„± ì–¸ì–´", ["ko-KR", "en-US", "ja-JP", "zh-CN"])
    voice_gender = st.selectbox("ì„±ë³„", ["ì—¬ì„±", "ë‚¨ì„±"])
    voice_speed = st.slider("ìŒì„± ì†ë„ (ê¸°ë³¸: 0)", min_value=-50, max_value=50, step=10, value=0)

# ì‚¬ìš©ìê°€ ë°”ê¾¼ ê°’ ì €ì¥
st.session_state["saved_voice"]["lang"] = voice_lang
st.session_state["saved_voice"]["gender"] = voice_gender
st.session_state["saved_voice"]["speed"] = voice_speed

# Create two columns for input methods
col1, col2 = st.columns(2)

with col1:
    st.markdown("### ğŸ¤ ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°")
    text_result = mic_recorder(
        start_prompt="ğŸ™ï¸ ë…¹ìŒ ì‹œì‘",
        stop_prompt="â¹ï¸ ë…¹ìŒ ì¤‘ì§€",
        use_container_width=True,
        just_once=True,
        key="mic"
    )

with col2:
    st.markdown("### âœï¸ í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸í•˜ê¸°")
    text_input = st.text_input(
        "ì…ë ¥í•˜ê³  ì‹¶ì€ ë‚´ìš©ì´ ìˆë‚˜ìš”?",
        placeholder="ì˜ˆ: ìˆ˜í•™ì´ ë„ˆë¬´ ì–´ë ¤ì›Œìš” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?"
    )

# Image upload
st.markdown("---")
st.markdown("### ğŸ–¼ï¸ ë¬¸ì œë‚˜ í•™ìŠµ ìë£Œê°€ ìˆë‹¤ë©´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”")
image_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg"])
image_base64 = None
if image_file is not None:
    image = Image.open(image_file)
    st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)
    image_base64 = base64.b64encode(image_file.getvalue()).decode("utf-8")

# edge-tts ë™ê¸° ë˜í¼ í•¨ìˆ˜
def speak_sync(text, filename="edge_output.mp3"):
    async def speak(text, filename):
        communicate = edge_tts.Communicate(text=text, voice="ko-KR-SunHiNeural")
        await communicate.save(filename)
    asyncio.run(speak(text, filename))

# ì‘ë‹µ ì €ì¥ìš© ì„¸ì…˜ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Voice output functions
def get_voice_id(lang="ko-KR", gender="ì—¬ì„±"):
    voices = {
        "ko-KR": {"ì—¬ì„±": "ko-KR-SunHiNeural", "ë‚¨ì„±": "ko-KR-InJoonNeural"},
    }
    return voices[lang][gender]

async def generate_tts(text, lang="ko-KR", gender="ì—¬ì„±", speed=0):
    voice_id = get_voice_id(lang, gender)
    rate = f"{speed:+d}%"
    communicate = edge_tts.Communicate(text=text, voice=voice_id, rate=rate)
    await communicate.save("tts_output.mp3")
    return "tts_output.mp3"

# GPT consultation with improved UI
if st.button("ğŸ“© ê³µìŒ¤ì—ê²Œ ì§ˆë¬¸ ë³´ë‚´ê¸°", use_container_width=True):
    user_question = text_result if text_result else text_input
    if not user_question:
        st.warning("âš ï¸ ë¨¼ì € ì§ˆë¬¸ì„ ë§í•˜ê±°ë‚˜ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    else:
        with st.spinner("ğŸ¤” ê³µìŒ¤ì´ ì—´ì‹¬íˆ ìƒê°í•˜ê³  ìˆì–´ìš”..."):
            prompt = f"ë„ˆëŠ” ì´ë¦„ì´ 'ê³µìŒ¤'ì¸ AI ì„ ìƒë‹˜ì´ì•¼. í•™ìŠµìì˜ ìˆ˜ì¤€ì€ {selected_level}ì´ê³ , ê³µë¶€ì— ëŒ€í•œ ê³ ë¯¼ì„ ì¹œì ˆí•˜ê³  ë”°ëœ»í•˜ê²Œ ìƒë‹´í•´ì¤˜."
            messages = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_question}
            ]
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=messages
            )
            answer = response.choices[0].message.content
            
            # Response container with improved styling
            st.markdown('<div class="response-container">', unsafe_allow_html=True)
            st.markdown("### ğŸ“£ ê³µìŒ¤ì˜ ëŒ€ë‹µ")
            st.markdown(answer)
            st.markdown('</div>', unsafe_allow_html=True)

            # Voice output with improved UI
            with st.spinner("ğŸ”Š ìŒì„± ë³€í™˜ ì¤‘..."):
                loop = asyncio.get_event_loop()
                audio_path = loop.run_until_complete(generate_tts(answer))
                st.audio(audio_path, format="audio/mp3")

st.markdown("### ğŸ“‚ ì˜¤ëŠ˜ì˜ ìƒë‹´ ê¸°ë¡ ë³´ê¸°")
for i, chat in enumerate(st.session_state.chat_history):
    st.markdown(f"**{i+1}. ì§ˆë¬¸:** {chat['ì§ˆë¬¸']}")
    st.markdown(f"ğŸ‘‰ **AI ì‘ë‹µ:** {chat['ë‹µë³€']}")
    st.markdown("---")

