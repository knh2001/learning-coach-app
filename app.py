import streamlit as st
import openai
from streamlit_mic_recorder import mic_recorder
from PIL import Image
import os
import base64
import edge_tts
import asyncio

# OpenAI
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Google Speech or Edge
GOOGLE_KEY = st.secrets["GOOGLE_API_KEY"]
AZURE_KEY = st.secrets["AZURE_SPEECH_KEY"]
AZURE_REGION = st.secrets["AZURE_REGION"]

st.set_page_config(page_title="ğŸ“š ì´ˆê°œì¸í™” í•™ìŠµ ìƒë‹´", layout="wide")
st.title("ğŸ“ AI ê¸°ë°˜ ì´ˆê°œì¸í™” í•™ìŠµ ìƒë‹´")

# ìë™ ì„¤ì • ê¸°ë³¸ê°’
if level in ["ìœ ì¹˜ì›", "ì´ˆë“±í•™ìƒ"]:
    default_voice_lang = "ko-KR"
    default_voice_gender = "ì—¬ì„±"
    default_voice_speed = 10
elif level in ["ì¤‘í•™ìƒ", "ê³ ë“±í•™ìƒ"]:
    default_voice_lang = "ko-KR"
    default_voice_gender = "ë‚¨ì„±"
    default_voice_speed = 0
else:
    default_voice_lang = "ko-KR"
    default_voice_gender = "ì—¬ì„±"
    default_voice_speed = 0

# Sidebar for user profile
with st.sidebar:
    st.header("ğŸ‘¤ í•™ìŠµì ì •ë³´")
    name = st.text_input("ì´ë¦„")
    level = st.selectbox("í•™ìŠµ ìˆ˜ì¤€", ["ìœ ì¹˜ì›", "ì´ˆë“±í•™ìƒ", "ì¤‘í•™ìƒ", "ê³ ë“±í•™ìƒ"])
    subject = st.multiselect("ê´€ì‹¬ ê³¼ëª©", ["ìˆ˜í•™", "ê³¼í•™", "êµ­ì–´", "ì˜ì–´"])
    mood = st.radio("ì˜¤ëŠ˜ ê¸°ë¶„ì€?", ["ğŸ™‚ ê´œì°®ì•„ìš”", "ğŸ˜ ë³´í†µì´ì—ìš”", "ğŸ˜£ ì¢€ í˜ë“¤ì–´ìš”"])
    st.markdown("---")
    st.subheader("ğŸ”ˆ ìŒì„± ì¶œë ¥ ì„¤ì •")
    voice_lang = st.selectbox("ìŒì„± ì–¸ì–´", ["ko-KR", "en-US", "ja-JP", "zh-CN"])

    gender_options = {
        "ko-KR": ["ì—¬ì„±", "ë‚¨ì„±"],
        "en-US": ["ì—¬ì„±", "ë‚¨ì„±"],
        "ja-JP": ["ì—¬ì„±", "ë‚¨ì„±"],
        "zh-CN": ["ì—¬ì„±", "ë‚¨ì„±"]
    }
    voice_gender = st.radio("ì„±ë³„", gender_options[voice_lang], horizontal=True)

    voice_speed = st.slider("ìŒì„± ì†ë„ (ê¸°ë³¸: 0)", min_value=-50, max_value=50, step=10, value=0)

# ì„¸ì…˜ ìƒíƒœì— ìŒì„± ì„¤ì • ì €ì¥
if "saved_voice" not in st.session_state:
    st.session_state["saved_voice"] = {
        "lang": default_voice_lang,
        "gender": default_voice_gender,
        "speed": default_voice_speed
    }

# ì‚¬ìš©ìê°€ ë°”ê¾¼ ê°’ ì €ì¥
st.session_state["saved_voice"]["lang"] = voice_lang
st.session_state["saved_voice"]["gender"] = voice_gender
st.session_state["saved_voice"]["speed"] = voice_speed

# Input zone: multimodal options
st.markdown("### ğŸ¤ ì§ˆë¬¸ì„ ë§í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•´ë³´ì„¸ìš”")

# Mic input
text_result = mic_recorder(start_prompt="ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°", stop_prompt="ì •ì§€", just_once=True, use_container_width=True, key="voice")

# Text input fallback
text_input = st.text_area("ë˜ëŠ” í…ìŠ¤íŠ¸ë¡œ ì§ˆë¬¸í•˜ê¸°", placeholder="í•™ìŠµì— ëŒ€í•´ ê¶ê¸ˆí•œ ê±¸ ììœ ë¡­ê²Œ ì ì–´ë³´ì„¸ìš”...", height=100)

# Image upload
st.markdown("---")
st.markdown("### ğŸ–¼ï¸ ë¬¸ì œë‚˜ í•™ìŠµ ìë£Œê°€ ìˆë‹¤ë©´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”")
image_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["png", "jpg", "jpeg"])
image_base64 = None
if image_file is not None:
    image = Image.open(image_file)
    st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)
    image_base64 = base64.b64encode(image_file.getvalue()).decode("utf-8")

# edge-tts ë™ê¸° ë˜í¼ í•¨ìˆ˜
def speak_sync(text, filename="edge_output.mp3"):
    async def speak(text, filename):
        communicate = edge_tts.Communicate(text=text, voice="ko-KR-SunHiNeural")
        await communicate.save(filename)
    asyncio.run(speak(text, filename))

# ì‘ë‹µ ì €ì¥ìš© ì„¸ì…˜ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Generate GPT response
if st.button("ğŸ§  AI ìƒë‹´ ë°›ê¸°"):
    with st.spinner("AIê°€ ìƒê° ì¤‘ì´ì—ìš”..."):
        user_content = text_result if text_result else text_input
        if not user_content:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ ë§í•´ì£¼ì„¸ìš”.")
        else:
            messages = [
                {"role": "system", "content": f"ë„ˆëŠ” {level} í•™ìŠµìì˜ ì§ˆë¬¸ì— ë§ëŠ” í•™ìŠµ ì „ëµê³¼ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ì´ˆê°œì¸í™” AI ë©˜í† ì•¼. ê´€ì‹¬ ê³¼ëª©ì€ {', '.join(subject)}ì•¼."},
                {"role": "user", "content": user_content},
            ]
            if image_base64:
                messages.append({
                    "role": "user",
                    "content": [
                        {"type": "text", "text": user_content},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                    ]
                })
            response = openai.ChatCompletion.create(
                model="gpt-4o",
                messages=messages
            )
            reply = response.choices[0].message.content
            st.success("AIì˜ ì‘ë‹µ:")
            st.markdown(reply)

            # ìƒë‹´ ê²°ê³¼ ì €ì¥
            if reply:
                st.session_state.chat_history.append({
                    "ì´ë¦„": name,
                    "í•™ìŠµ ìˆ˜ì¤€": level,
                    "ê¸°ë¶„": mood,
                    "ê³¼ëª©": subject,
                    "ì§ˆë¬¸": user_content,
                    "ë‹µë³€": reply
                })

            # ìŒì„± ì¶œë ¥ (Edge TTS)
            audio_path = asyncio.run(generate_tts(reply, lang=voice_lang, gender=voice_gender, speed=voice_speed))
            st.audio(audio_path, format="audio/mp3")

default_voice_lang = st.session_state["saved_voice"]["lang"]
default_voice_gender = st.session_state["saved_voice"]["gender"]
default_voice_speed = st.session_state["saved_voice"]["speed"]

st.markdown("### ğŸ“‚ ì˜¤ëŠ˜ì˜ ìƒë‹´ ê¸°ë¡ ë³´ê¸°")
for i, chat in enumerate(st.session_state.chat_history):
    st.markdown(f"**{i+1}. ì§ˆë¬¸:** {chat['ì§ˆë¬¸']}")
    st.markdown(f"ğŸ‘‰ **AI ì‘ë‹µ:** {chat['ë‹µë³€']}")
    st.markdown("---")

